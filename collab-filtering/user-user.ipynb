{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we implement a user-based collaborative filtering, trained with the subset of data (see \"preprocessing_final.ipynb\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from sortedcontainers import SortedList\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now load the data (takes about 10s)\n",
    "with open('user2movie.json', 'rb') as f:\n",
    "  user2movie = pickle.load(f)\n",
    "with open('movie2user.json', 'rb') as f:\n",
    "  movie2user = pickle.load(f)\n",
    "with open('usermovie2rating.json', 'rb') as f:\n",
    "  usermovie2rating = pickle.load(f)\n",
    "with open('usermovie2rating_test.json', 'rb') as f:\n",
    "  usermovie2rating_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.max(list(user2movie.keys())) + 1 # N = number of users\n",
    "# test set may contain movies that train set doesn't have data on\n",
    "m1 = np.max(list(movie2user.keys()))\n",
    "m2 = np.max([m for (u,m),r in usermovie2rating_test.items()])\n",
    "M = max(m1,m2) + 1\n",
    "print(\"N =\",N)\n",
    "print(\"M =\",M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (894793694.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_37928/894793694.py\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    movies_j_unique= = set(movies_j)\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# the calculation of user similarities is O(M*N^2)\n",
    "# in the \"real-world\",  we'd parallelize this (say, with PySpark)\n",
    "# since the weights are symmetric (w_ij = w_ji) we really only need to do half the calculations... but the price to be paid would be memory =/ \n",
    "\n",
    "K = 25 # number of neighbors we'd like to consider\n",
    "limit = 5 # minimum number of common movies users must have in common in order to the weights be relevant\n",
    "neighbors = [] # we'll store neighbors in this list\n",
    "averages = [] # each user's average rating for later use\n",
    "deviations = [] # idem above, now for user's deviation\n",
    "\n",
    "for j in range(N):\n",
    "  # find the K closest users to user i\n",
    "  movies_j = user2movie[j]\n",
    "  movies_j_unique= = set(movies_j)\n",
    "  \n",
    "  # average and deviation...\n",
    "  ratings_j = {movie:usermovie2rating[(j, movie)] for movie in movies_j}\n",
    "  avg_j = np.mean(list(ratings_j.values()))\n",
    "  dev_j = {movie: (rating - avg_j) for movie,rating in ratings_j.items()}\n",
    "  dev_j_values = np.array(list(dev_j.values()))\n",
    "  sigma_i = np.sqrt(dev_j_values.dot(dev_j_values))\n",
    "  averages.append(avg_j)\n",
    "  deviations.append(dev_j)\n",
    "  \n",
    "  sl = SortedList()\n",
    "  for k in range(N):\n",
    "    # don't include j itself!\n",
    "    if k != j:\n",
    "      movies_k = user2movie[k]\n",
    "      movies_k_set = set(movies_k)\n",
    "      common_movies = (movies_j_set & movies_k_set) # find their intersection\n",
    "      if len(common_movies) > limit:\n",
    "        # obtain the average and deviation, now for k\n",
    "        ratings_k = {movie:usermovie2rating[(k, movie)] for movie in movies_k}\n",
    "        avg_k = np.mean(list(ratings_k.values()))\n",
    "        dev_k = {movie: (rating - avg_k) for movie,rating in ratings_k.items()}\n",
    "        dev_k_values = np.array(list(dev_k.values()))\n",
    "        sigma_k = np.sqrt(dev_k_values.dot(dev_k_values))\n",
    "        \n",
    "        # now, compute the correlation\n",
    "        numerator = sum(dev_j[m]*dev_k[m] for m in common_movies)\n",
    "        w_jk=numerator/(sigma_j*sigma_k)\n",
    "        \n",
    "        # insert into sorted list and truncate\n",
    "        # negate weight, since list is sorted ascending\n",
    "        # max (1) is \"closest\"\n",
    "        sl.add(-w_jk, k)\n",
    "        if len(sl) > K:\n",
    "          del sl[-1]\n",
    "  # then, we store the neighbors\n",
    "  neighbors.append(sl)\n",
    "  print(\"j =\",j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now, we calculate train and test MSE\n",
    "def predict(n,m):\n",
    "  numerator=0\n",
    "  denominator=0\n",
    "  for neg_w, j in neighbors[n]:\n",
    "    # weight is stored as its negative, remember that!\n",
    "    try:\n",
    "      numerator += -neg_w*deviations[j][m]\n",
    "      denominator += abs(neg_w)\n",
    "    except KeyError:\n",
    "      # neighbor didn't rate same movie\n",
    "      # to avoid 2 lookups, we just throw the exception and pass\n",
    "      pass\n",
    "  if denominator == 0:\n",
    "    prediction = averages[n]\n",
    "  else:\n",
    "    prediction = (numerator/denominator) + averages[n]\n",
    "  prediction = min(5, prediction)\n",
    "  prediction = max(0.5, prediction)\n",
    "  return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us now train and test!\n",
    "train_preds = []\n",
    "train_targets = []\n",
    "for (n,m), target in usermovie2rating.items():\n",
    "  prediction = predict(n,m)\n",
    "  train_preds.append(prediction)\n",
    "  train_targets.append(target)\n",
    "\n",
    "test_preds = []\n",
    "test_targets = []\n",
    "\n",
    "for (n,m), target in usermovie2rating_test.items():\n",
    "  prediction = predict(n,m)\n",
    "  test_preds.append(prediction)\n",
    "  test_targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we calculate the mse accuracy\n",
    "def MSE(p,t):\n",
    "  p = np.array(p)\n",
    "  t = np.array(t)\n",
    "  return np.mean((p-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train MSE:\", MSE(train_preds, train_targets))\n",
    "print(\"Test MSE:\", MSE(test_preds, test_targets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
